---
title: "Creating triggers with EventArc - The Google Cloud wonder"
datePublished: Wed Jun 12 2024 18:51:38 GMT+0000 (Coordinated Universal Time)
cuid: clxc6t846000708l52a0bct4a
slug: creating-triggers-with-eventarc-the-google-cloud-wonder
cover: https://cdn.hashnode.com/res/hashnode/image/upload/v1717793465259/63a7fe9d-f048-4b9a-99ad-94ec3c388b18.jpeg
tags: cloud-computing, gcp, cloudrun, triggers, eventarc

---

Technology is evolving so rapidly that if we look at the cloud providers today, we may get at least 3 services per provider. After doing the hustle with AWS and Azure, it is now time to explore GCP (aka Google Cloud Platform).

# Introduction

Stateless containers can be operated in a fully managed environment using Cloud Run. Because it is based on the open-source Knative, you may use Cloud Run for Anthos to run your containers in your Google Kubernetes Engine cluster or fully manage them with Cloud Run.

Connecting different services (Cloud Run, Cloud Functions, Workflows) with events from many sources is made simple by Eventarc. It enables you to create loosely linked, distributed microservices within event-driven architectures. Enhancing developer agility and application resilience, it also handles event ingestion, delivery, security, authorization, and error-handling for you.

You will become familiar with Eventarc in this codelab. More precisely, you will use Eventarc to listen for events from Pub/Sub, Cloud Storage, and Cloud Audit Logs and then send those events to a Cloud Run service.  

# Eventarc Vision

The goal of Eventarc is to transport events to Google Cloud event destinations from a variety of Google, Google Cloud, and third-party event sources.  

![59b147dc030b2b0b.png](https://codelabs.developers.google.com/static/codelabs/cloud-run-events/img/59b147dc030b2b0b.png?authuser=1 align="left")

Image Source: [https://codelabs.developers.google.com/codelabs/cloud-run-events?authuser=1#1](https://codelabs.developers.google.com/codelabs/cloud-run-events?authuser=1#1)

Let me tell you about these blocks:

## Google Sources

Event sources that are controlled by Google, including Android Management, Hangouts, Gmail, and more.

## Google Cloud Sources

Event sources that Google Cloud controls.

## Custom Sources

Sources of events that are produced by end users and are not held by Google

## 3rd Party Sources

Sources of events that are neither created by customers nor controlled by Google. This contains well-known event sources owned and maintained by partners and third-party providers, such as Check Point CloudGuard, Datadog, ForgeRock, Lacework, etc.

To enable cross-service interoperability, events are normalized to the CloudEvents v1.0 standard. Interoperability between services, platforms, and systems is made possible by CloudEvents, an open specification that is vendor-neutral and describes event data in standard formats.

# Setup and requirements

I am doing this all by self, so I started with the account and new project creation.

## Creating a new account for google cloud

Head to the Google Cloud console using: [https://console.cloud.google.com](https://console.cloud.google.com/welcome?authuser=0&project=kubernetes-codelab-425719)/

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1717845014280/b5bf7e5e-7e40-4210-8940-40d3a753b88c.png align="center")

The next thing is to create a new project. So Click on the "My first Project" which is located right next to the Google Cloud on the left side top corner and provide a project name. Click on create.

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1717845020024/4e3b7c28-cf1a-4080-adcb-9c07f965f85d.png align="center")

Once the project is created, Click on the select project from the option shown

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1717845029458/3c2e8cf5-d6fc-44a4-b616-2dbc460f16df.png align="center")

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1717845039824/8873ae82-e0d6-401a-a8e7-8d0a07b58289.png align="center")

## Start the cloudshell

Navigate to the cloudshell using the option from the console shown below:

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1717917348244/0016ef32-2179-4294-84d3-b2f0c61409dc.png align="center")

You should be able to see the screen below with the terminal open

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1717917379545/64a47019-824a-47fe-b1f4-f187ff286d57.png align="center")

When the shell is connected, you will see something like this:

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1717917402231/2b747525-abdf-46f0-9a5e-6529ad06ef55.png align="center")

We need to connect to the project first so that we can start working on the project infrastructure. So execute the commands below:

> PROJECT\_ID=your-project-id
> 
> gcloud config set project $PROJECT\_ID

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1717918270806/41ee2780-6be3-474c-87ca-95f3ce75a472.png align="center")

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1717918295814/1871c3f7-ae9c-40bb-9f50-bddf3cfb206f.png align="center")

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1717918306279/f21f05e8-1160-4b7c-b2f6-c6c0345b9292.png align="center")

## Deploy a Cloud Run service

Since we are looking at a PUBSUB architecture, there should be a sender and a receiver. We will set up the Cloud Run service to receive the events. First, let us deploy, Cloud Run's Hello container as a basic that logs the contents of CloudEvents.

Let's enable the required services for the Cloud Run:

> gcloud services enable [run.googleapis.com](http://run.googleapis.com)

Deploy the Hello container:

> REGION=us-central1
> 
> SERVICE\_NAME=hello
> 
> gcloud run deploy $SERVICE\_NAME  
> \--allow-unauthenticated  
> \--image=[gcr.io/cloudrun/hello](http://gcr.io/cloudrun/hello)  
> \--region=$REGION

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1717918322649/7980bd1e-6752-4c5d-849f-baa1cd4c9890.png align="center")

Once the deployment is complete, you should see the status like this:

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1717918333194/893ebcae-aec9-4608-adca-ede017e5819d.png align="center")

Alright! time for a quick test of the URL. Hit the URL from the browser to see the deployment status:

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1717918398351/89caa0b4-1a04-47ad-85b1-af6b4b10eb4c.png align="center")

Great! It works!

## Event Discovery

You may learn about the event sources in Eventarc, the kinds of events they can produce, and how to set up triggers to ingest them before you start generating triggers. To understand more on these commands, you can visit the official documentation from google which provides updated information on the commands:

[https://cloud.google.com/sdk/gcloud/reference/eventarc](https://cloud.google.com/sdk/gcloud/reference/eventarc)

## Creating a PubSub Trigger

Using Cloud Pub/Sub is one method of receiving events. Any application can send messages to Pub/Sub, and Eventarc can transmit these messages to Cloud Run. Let's head to the console and create a trigger.

Before creating any triggers, we need to enable a few services for eventarc. Let us enable the services for apis, create and configure the service account and then create and configure the trigger. Here are the commands:

> #Enable the API services
> 
> gcloud services enable [eventarc.googleapis.com](http://eventarc.googleapis.com)
> 
> #Create and configure service account
> 
> SERVICE\_ACCOUNT=eventarc-trigger-sa
> 
> gcloud iam service-accounts create $SERVICE\_ACCOUNT
> 
> #Create and configure the trigger
> 
> TRIGGER\_NAME=trigger-pub-subz
> 
> gcloud eventarc triggers create $TRIGGER\_NAME  
> \--destination-run-service=$SERVICE\_NAME  
> \--destination-run-region=$REGION  
> \--event-filters="type=[google.cloud](http://google.cloud).pubsub.topic.v1.messagePublished"  
> \--location=$REGION  
> \--service-account=$SERVICE\_ACCOUNT@$PROJECT\_[ID.iam.gserviceaccount.com](http://ID.iam.gserviceaccount.com)

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718002706298/8eaead9d-7f80-49bf-ab6f-474c6a135f19.png align="center")

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718002712026/c7b4cf29-8d60-4d6a-a1f3-097f2a6d465b.png align="center")

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718002716881/3c9ea9d4-0fde-495d-8c98-96cd9de240dc.png align="center")

Let us test this deployment now.

> #Test the configuration
> 
> TOPIC\_ID=$(gcloud eventarc triggers describe $TRIGGER\_NAME --location $REGION --format='value(transport.pubsub.topic)')
> 
> gcloud pubsub topics publish $TOPIC\_ID --message="Hello World"

The body of the incoming message is logged by the Cloud Run service. This is shown in your Cloud Run instance's Logs section:

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718002868121/649b4686-5fc8-4ffe-bc41-dcff039d0f0b.png align="center")

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718002874381/4e13d114-5903-40c9-882e-6b1e31c44074.png align="center")

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718002879240/0c7b8d6f-1990-4359-8349-1ab293f1eb94.png align="center")

Creating with an existing Pub/Sub Topic

Eventarc automatically generates a topic under the covers when you create a Pub/Sub trigger, which you can use as a transport topic between your application and a Cloud Run service. While it is helpful to quickly and simply establish a Pub/Sub backed trigger, there are situations when you might wish to use an already-existing subject. With the --transport-topic gcloud argument, Eventarc lets you provide an already-existing Pub/Sub topic within the same project.

Creating a topic:

> TOPIC\_ID=eventarc-topic
> 
> gcloud pubsub topics create $TOPIC\_ID

Create a trigger:

> TRIGGER\_NAME=trigger-pubsub-existing
> 
> gcloud eventarc triggers create $TRIGGER\_NAME  
> \--destination-run-service=$SERVICE\_NAME  
> \--destination-run-region=$REGION  
> \--event-filters="type=[google.cloud](http://google.cloud).pubsub.topic.v1.messagePublished"  
> \--location=$REGION  
> \--transport-topic=projects/$PROJECT\_ID/topics/$TOPIC\_ID  
> \--service-account=$SERVICE\_ACCOUNT@$PROJECT\_[ID.iam.gserviceaccount.com](http://ID.iam.gserviceaccount.com)

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718198956692/c7e1a2bc-bacd-4834-a394-3bdbdc785e33.png align="center")

Let us send a test message and see if this really worked.

> gcloud pubsub topics publish $TOPIC\_ID --message="Hello again"

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718199058975/ab8a4046-2deb-44ae-a3cd-f94094014423.png align="center")

Great! That worked! There should be a message id that should get created.

# Creating a Cloud Storage Trigger

When you create a trigger and start sending messages, there should be a listener or a receiver. Let us create a trigger to listen for the events from the Cloud Storage.

> BUCKET\_NAME=eventarc-gcs-$PROJECT\_ID
> 
> gsutil mb -l $REGION gs://$BUCKET\_NAME

The above commands mentioned will create a bucket to receive events.

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718199380451/812b9ce3-3fff-4b83-b0a6-a6b1d8d96619.png align="center")

We need to grant eventarc a role to receive... which is everntarc\_eventReciever sot that the service account can be used in a Cloud Storage Trigger:

> gcloud projects add-iam-policy-binding $PROJECT\_ID  
> \--role roles/eventarc.eventReceiver  
> \--member serviceAccount:$SERVICE\_ACCOUNT@$PROJECT\_[ID.iam.gserviceaccount.com](http://ID.iam.gserviceaccount.com)

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718199394369/a83edd30-f10b-4a67-8388-c10fe22d4ab8.png align="center")

In order to use Cloud Storage triggers with the Cloud Storage service account, you must further add the pubsub.publisher role:

> SERVICE\_ACCOUNT\_STORAGE=$(gsutil kms serviceaccount -p $PROJECT\_ID)
> 
> gcloud projects add-iam-policy-binding $PROJECT\_ID  
> \--member serviceAccount:$SERVICE\_ACCOUNT\_STORAGE  
> \--role roles/pubsub.publisher

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718199743487/1f3861c7-5971-4de1-a92d-caf9705c4be4.png align="center")

Great! we created a bucket and created a service account with the role as well. Let's create a trigger to route the new file creation events from the bucket to the service.

> TRIGGER\_NAME=trigger-storage
> 
> gcloud eventarc triggers create $TRIGGER\_NAME  
> \--destination-run-service=$SERVICE\_NAME  
> \--destination-run-region=$REGION  
> \--event-filters="type=[google.cloud.storage](http://google.cloud.storage).object.v1.finalized"  
> \--event-filters="bucket=$BUCKET\_NAME"  
> \--location=$REGION  
> \--service-account=$SERVICE\_ACCOUNT@$PROJECT\_[ID.iam.gserviceaccount.com](http://ID.iam.gserviceaccount.com)

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718199762070/57beb10c-a2a9-45bc-9190-b09f05dbb60b.png align="center")

Let us take a look at all the triggers we created

> gcloud eventarc triggers list

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718200601307/ca52e9f1-28d9-45bb-a32b-e6b7bd48c74b.png align="center")

That gives us all the details about the triggers and their status.

Let's upload the file to the Cloud Storage Bucket:

> echo "Hello World" &gt; random.txt
> 
> gsutil cp random.txt gs://$BUCKET\_NAME/random.txt

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718200767115/e517c04c-96e0-4efc-9d8e-847692fd03c8.png align="center")

To check what happened when we ran this command, we can check the logs of the Cloud Run service in the cloud console. we should be able to see this event received.

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718200781421/bb28071d-7da6-4ff4-9c16-d1298e16af53.png align="center")

One last part before we run to the GUI of the eventarc. Creating an audit log logs trigger and testing the same.

# Create a Cloud Audit Logs trigger

In this step, you establish a Cloud Audit Log trigger to perform the same thing, even though the Cloud Storage trigger is the better approach to listen for Cloud Storage events.

You have to enable Cloud Audit Logs before you can get events from a service. Select Audit Logs and IAM & Admin from the Cloud Console's top left menu. Go to the services list and look for Google Cloud Storage.

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718215088358/935461be-65c2-40ab-997f-a10812bac867.png align="center")

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718215093499/d93213a2-8a05-4b06-982e-ef30e08a04f4.png align="center")

Make sure Admin, Read, and Write are selected on the right-hand side, then click Save:  

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718215232932/496921e9-f68e-421c-ae06-328e202c4409.png align="center")

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718215172294/5cafb6f5-9aa5-45c7-8994-bb7260df2248.png align="center")

Once this is set, let us create a trigger to route new file creation events from bucket to our service:

> TRIGGER\_NAME=trigger-auditlog-storage
> 
> gcloud eventarc triggers create $TRIGGER\_NAME  
> \--destination-run-service=$SERVICE\_NAME  
> \--destination-run-region=$REGION  
> \--event-filters="type=[google.cloud](http://google.cloud).audit.log.v1.written"  
> \--event-filters="serviceName=[storage.googleapis.com](http://storage.googleapis.com)"  
> \--event-filters="methodName=storage.objects.create"  
> \--event-filters-path-pattern="resourceName=/projects/\_/buckets/$BUCKET\_NAME/objects/\*"  
> \--location=$REGION  
> \--service-account=$SERVICE\_ACCOUNT@$PROJECT\_[ID.iam.gserviceaccount.com](http://ID.iam.gserviceaccount.com)

<div data-node-type="callout">
<div data-node-type="callout-emoji">💡</div>
<div data-node-type="callout-text">Eventarc supports applying a path pattern when filtering. For example, you can filter on the optional <strong>resourceName</strong> field. In this case, you are filtering for all object creations under a specific bucket.</div>
</div>

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718215430393/e131ccbd-6992-4c98-afb0-2453589a7a36.png align="center")

Let's quickly check if the triggers have been created:

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718215447273/569d8f95-4ba1-43b7-b293-9b2895313dc5.png align="center")

Let us upload the same file we used earlier to the Cloud Storage Bucket again:

> gsutil cp random.txt gs://$BUCKET\_NAME/random.txt

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718215854583/d4aceb25-fe1b-49e1-8ed8-c8ca15d80c91.png align="center")

Let us go to Cloud Run and check the received event:

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718217266675/bd283c85-3367-4c6c-9dcc-7d38288235f9.png align="center")

Great! we can see the event successfully.

One last topic before we can conclude on this is the GUI...

# The GUI

Let's head to the GUI to see what all we have created using the CLI.

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718217656522/1e6a3d30-3579-4ccd-9935-d1cd2d9255b1.png align="center")

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718217664645/3c50913c-48ad-4662-ae6f-cef40bf2b9de.png align="center")

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718217672370/d91bc1a2-b9c6-4935-bde0-dc5a18d59e41.png align="center")

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718217678774/b88a5fa2-6660-438e-a648-0d9052daa12b.png align="center")

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718217685458/b9411a39-eb59-4966-a609-f9ec76f9a77e.png align="center")

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718217692812/ec4c3b16-8bf6-4f70-9412-99f2d3019a66.png align="center")

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718217701404/69e64bc1-f290-41a8-b629-832804ccb684.png align="center")

That's how we can create a trigger from the GUI...

# Learning from this project

Here is what we learned:

* Vision of Eventarc
    
* Discovering events in Eventarc
    
* Creating a Cloud Run sink
    
* Creating a trigger for Pub/Sub
    
* Creating a trigger for Cloud Storage
    
* Creating a trigger for Cloud Audit Logs
    
* The Eventarc UI
    

Last but not least, make sure that we shutdown the resources.

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718218144623/72ca6642-d251-47cb-9252-f3e16ac8fb91.png align="center")

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718218150458/51fc031d-c871-4c87-bc81-ef6a5ffac4f9.png align="center")

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718218154777/e53c9d9b-2f65-4ee8-8c88-43c60f44eea3.png align="center")

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718218161313/538dc225-383a-41bc-a992-f09562460a6b.png align="center")

Happy learning!